{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/revatishelat/DST_A2/blob/main/report/05_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA_1En9CqlGs"
      },
      "source": [
        "#### 1. Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCt9Yisrjamm"
      },
      "source": [
        "If the any of the libraries are not installed, please use !pip install [name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwzwv0W3UGFy",
        "outputId": "0492b4b8-aa87-4c43-8c6f-2eeaa4a102d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#preprocessing\n",
        "import pandas as pd\n",
        "\n",
        "import re #for regular expression\n",
        "import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "#for rnn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGSOGDisqoJL"
      },
      "source": [
        "#### 2. Load datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP-ua3PCjnzd"
      },
      "source": [
        "The training and test datasets that are used in this script are available [here](https://colab.research.google.com/drive/1tr7CoaE6InDuI445Lj1StgUWEB90DQOt#scrollTo=aP-ua3PCjnzd&line=1&uniqifier=1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gSjjobxZUGFz"
      },
      "outputs": [],
      "source": [
        "# Load the data from CSV\n",
        "train_df =pd.read_csv(\"https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/main/medical_tc_train.csv\")\n",
        "test_df =pd.read_csv(\"https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/main/medical_tc_test.csv\")\n",
        "labels = pd.read_csv(\"https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/main/medical_tc_labels.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogJZkEcXVVym",
        "outputId": "bd6197c2-4a72-4432-e70e-cd71af393eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first few rows of the training dataset:     condition_label                                   medical_abstract\n",
            "0                5  Tissue changes around loose prostheses. A cani...\n",
            "1                1  Neuropeptide Y and neuron-specific enolase lev...\n",
            "2                2  Sexually transmitted diseases of the colon, re...\n",
            "3                1  Lipolytic factors associated with murine and h...\n",
            "4                3  Does carotid restenosis predict an increased r...\n",
            "5                3  The shoulder in multiple epiphyseal dysplasia....\n",
            "6                2  The management of postoperative chylous ascite...\n",
            "7                4  Pharmacomechanical thrombolysis and angioplast...\n",
            "8                5  Color Doppler diagnosis of mechanical prosthet...\n",
            "9                5  Noninvasive diagnosis of right-sided extracard...\n"
          ]
        }
      ],
      "source": [
        "print(\"The first few rows of the training dataset: \", train_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGD3mSbzVXvQ",
        "outputId": "b07dffa1-a3a1-4973-cbd2-54dea95007b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first few rows of the test dataset:     condition_label                                   medical_abstract\n",
            "0                3  Obstructive sleep apnea following topical orop...\n",
            "1                5  Neutrophil function and pyogenic infections in...\n",
            "2                5  A phase II study of combined methotrexate and ...\n",
            "3                1  Flow cytometric DNA analysis of parathyroid tu...\n",
            "4                4  Paraneoplastic vasculitic neuropathy: a treata...\n",
            "5                1  Treatment of childhood angiomatous diseases wi...\n",
            "6                1  Expression of major histocompatibility complex...\n",
            "7                1  Questionable role of CNS radioprophylaxis in t...\n",
            "8                5  Reversibility of hepatic fibrosis in experimen...\n",
            "9                2  Current status of duplex Doppler ultrasound in...\n"
          ]
        }
      ],
      "source": [
        "print(\"The first few rows of the test dataset: \", test_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v3hlPrT8p3g"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "omB3628bG_mm"
      },
      "outputs": [],
      "source": [
        "# train_df[\"medical_abstract\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgI0xDLQyXur"
      },
      "source": [
        "#### 3. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg8vz7UJAS4X"
      },
      "source": [
        "The preprocessing of text is similar to that 01_Introduction_EDA_and_preprocessing.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "avdK9hR1YsGE"
      },
      "outputs": [],
      "source": [
        "#preprocessing function\n",
        "def preprocess(data):\n",
        "    processed_data = []\n",
        "    for doc in data['medical_abstract']:\n",
        "        #lowercases document\n",
        "        doc = doc.lower()\n",
        "        #removes any non-letter characters\n",
        "        doc = re.sub(r'\\b[^a-zA-Z]+\\b', ' ', doc)\n",
        "        #tokenize\n",
        "        toks = nltk.word_tokenize(doc)\n",
        "        #remove tokens of lenth <= 1 (can be varied)\n",
        "        toks = [tok for tok in toks if len(tok) > 1]\n",
        "        #remove stopwords\n",
        "        toks = [tok for tok in toks if tok not in en_stop]\n",
        "        #lemmatize\n",
        "        toks = [WordNetLemmatizer().lemmatize(tok) for tok in toks]\n",
        "        processed_data.append(toks)\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e3mq8RvQUGFz"
      },
      "outputs": [],
      "source": [
        "# Split the data into text and labels\n",
        "train_texts, y_train_labels = train_df['medical_abstract'], train_df['condition_label']\n",
        "test_texts, y_test_labels = test_df['medical_abstract'], test_df['condition_label']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nl2J9vPaKbjL"
      },
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "    for x in df: #['medical_abstract']:\n",
        "        #lowercases document\n",
        "        x = x.lower()\n",
        "        #removes any non-letter characters\n",
        "        x = re.sub(r'\\b[^a-zA-Z]+\\b', ' ', x)\n",
        "        #tokenize\n",
        "        toks = nltk.word_tokenize(x)\n",
        "        #remove tokens of lenth <= 1 (can be varied)\n",
        "        toks = [tok for tok in toks if len(tok) > 1]\n",
        "        #remove stopwords\n",
        "        toks = [tok for tok in toks if tok not in en_stop]\n",
        "        #lemmatize\n",
        "        toks = [WordNetLemmatizer().lemmatize(tok) for tok in toks]\n",
        "        return toks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NG36QDN_GK-L"
      },
      "outputs": [],
      "source": [
        "X_train_texts = preprocess(train_df)\n",
        "X_test_texts = preprocess(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IUgbgvnyStx0"
      },
      "outputs": [],
      "source": [
        "# len(train_texts)\n",
        "# type(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aTXOCMecUGFz"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text data\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z8I-TRC9UGF0"
      },
      "outputs": [],
      "source": [
        "# Convert text data to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9q_7j5dSVs4h"
      },
      "outputs": [],
      "source": [
        "# train_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8AcqbRDTV1ci"
      },
      "outputs": [],
      "source": [
        "# test_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VJnAksk1UGF0"
      },
      "outputs": [],
      "source": [
        "# Pad sequences to ensure consistent length\n",
        "max_length = 100\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vIn4saHAkE3"
      },
      "source": [
        "#### 4. LSTM (RNN) model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyFneAhXrlm5"
      },
      "source": [
        "In this section, we tokenize the training text, pad sequences, and build an RNN model with keras library. We first tokenize the training data. Here, words that are not in the tokenizer's word index are represented by out-of-vocabulary tokens (i.e. < OOV > ). Here, the internal vocaulary is updated based on the internal vocabulary of the training text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build a sequential model that has exactly one input and output tensor. We also add the LSTM layer which caputures the long-term dependencies in the sequences."
      ],
      "metadata": {
        "id": "egXefs1mcQu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import keras\n",
        "# def create_lstm_model(embedding_layer = None):\n",
        "#     # create input layer\n",
        "#     inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "#     # add word embedding layer\n",
        "#     if embedding_layer is not None:\n",
        "#         embedded = embedding_layer(inputs)\n",
        "#     else:\n",
        "#         embedded = layers.Embedding(input_dim=MAX_TOKENS, output_dim=256, mask_zero=True)(inputs)\n",
        "#     # add LSTM layer\n",
        "#     x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "#     # add dropout layer\n",
        "#     x = layers.Dropout(0.5)(x)\n",
        "#     # add output layer\n",
        "#     outputs = layers.Dense(9, activation=\"softmax\")(x)\n",
        "#     # combine all layers into one model\n",
        "#     lstm_model = keras.Model(inputs, outputs)\n",
        "#     # specifiy optimizer, loss, and metrics for the model\n",
        "#     lstm_model.compile(optimizer=\"rmsprop\",\n",
        "#                   loss=\"sparse_categorical_crossentropy\",\n",
        "#                   metrics=[\"accuracy\"])\n",
        "#     # print the summay of the model architecture\n",
        "#     lstm_model.summary()\n",
        "\n",
        "#     return lstm_model\n",
        "# if SEQUENCE_MODEL:\n",
        "#     if USE_GROVE:\n",
        "#         lstm_model = create_lstm_model(embedding_layer)\n",
        "#     else:\n",
        "#         lstm_model = create_lstm_model()\n",
        "\n",
        "#     # define callback function\n",
        "#     callbacks = [\n",
        "#         keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "#                                         save_best_only=False)\n",
        "#     ]\n",
        "#     # train model\n",
        "#     lstm_model.fit(X_train, y_train, epochs=10, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "vbC_qxOLaejI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e1A-QMLWJnCE"
      },
      "outputs": [],
      "source": [
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-usfdZRyAlCP"
      },
      "outputs": [],
      "source": [
        "# Pad sequences to ensure consistent length\n",
        "max_length = 100\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5r1b-qStNKAj"
      },
      "outputs": [],
      "source": [
        "#check for unique words in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qS91stvaUGF0"
      },
      "outputs": [],
      "source": [
        "# Build the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, input_length=max_length, output_dim=64))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAM8Gs0vZbtP"
      },
      "source": [
        "In order to correct the out of bounds error, we substract 1 from each label. So, now we have the corresponding classes:\\\n",
        "0 : Neoplasms\\\n",
        "1 : Digestive system diseases\\\n",
        "2 : Nervous system diseases\\\n",
        "3 : Cardiovascular diseases\\\n",
        "4 : General pathological conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_EKp27xRurSH"
      },
      "outputs": [],
      "source": [
        "# Enable GPU acceleration if available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU is available')\n",
        "    model = tf.keras.utils.multi_gpu_model(model, gpus=2)  # adjust the number of GPUs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mxQTVsjKCDiL"
      },
      "outputs": [],
      "source": [
        "# y_train_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "T-WdDMwSYiTc"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_train_labels -= 1\n",
        "y_test_labels -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EhNZfCFTZQsk"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(train_padded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piUCQkjzmCnF",
        "outputId": "2d111d47-bd77-4bf2-9508-651f460fe428"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11550"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzwHOd8kUGF0",
        "outputId": "ec65538f-cd70-491e-f59c-144f58fdfd72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "361/361 [==============================] - 58s 161ms/step - loss: 1.3697 - accuracy: 0.4273\n",
            "Epoch 2/10\n",
            "361/361 [==============================] - 61s 170ms/step - loss: 1.1352 - accuracy: 0.5549\n",
            "Epoch 3/10\n",
            "361/361 [==============================] - 69s 190ms/step - loss: 1.0760 - accuracy: 0.5813\n",
            "Epoch 4/10\n",
            "361/361 [==============================] - 65s 180ms/step - loss: 1.0576 - accuracy: 0.5861\n",
            "Epoch 5/10\n",
            "361/361 [==============================] - 58s 161ms/step - loss: 0.9474 - accuracy: 0.6258\n",
            "Epoch 6/10\n",
            "361/361 [==============================] - 66s 182ms/step - loss: 0.9319 - accuracy: 0.6272\n",
            "Epoch 7/10\n",
            "361/361 [==============================] - 63s 176ms/step - loss: 1.1777 - accuracy: 0.5379\n",
            "Epoch 8/10\n",
            "361/361 [==============================] - 58s 160ms/step - loss: 0.9545 - accuracy: 0.6242\n",
            "Epoch 9/10\n",
            "361/361 [==============================] - 58s 159ms/step - loss: 0.7843 - accuracy: 0.6859\n",
            "Epoch 10/10\n",
            "361/361 [==============================] - 67s 185ms/step - loss: 0.7144 - accuracy: 0.7059\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f3bbc42df00>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Train the model\n",
        "# time taken to for 10 epochs:\n",
        "model.fit(train_padded, y_train_labels, epochs=10)\n",
        "\n",
        "# model.compile(loss = CategoricalCrossentropy(), optimizer = Adam(), metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It takes ~11 minutes to fit the training the data for 10 epochs. We get an accuracy of ~70.59. This is quite low as compared to other LSTM models, which have accuracies ranging from 80-97. [See here](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-022-01665-y) and [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8486521/)"
      ],
      "metadata": {
        "id": "pVzpy69dmqHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKK71hVKUGF1",
        "outputId": "9d99069e-7f98-40ad-aa86-1a86abf02650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91/91 [==============================] - 5s 50ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on test data\n",
        "predictions = model.predict(test_padded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fCmAJZYDUGF1"
      },
      "outputs": [],
      "source": [
        "# Convert predictions to labels\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SB3qLOHDq6zM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C07ZtXTLUGF1",
        "outputId": "f0fce3e7-f869-425c-a228-ab3681953244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[437  30  21  20 125]\n",
            " [ 45 103   3  14 134]\n",
            " [ 40   9 128  48 160]\n",
            " [ 10   9  17 410 164]\n",
            " [163 119 127 249 303]]\n"
          ]
        }
      ],
      "source": [
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x8fYhWiUGF1",
        "outputId": "aa9381ea-91c1-445c-941a-5bb2af804856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.69      0.66       633\n",
            "           1       0.38      0.34      0.36       299\n",
            "           2       0.43      0.33      0.38       385\n",
            "           3       0.55      0.67      0.61       610\n",
            "           4       0.34      0.32      0.33       961\n",
            "\n",
            "    accuracy                           0.48      2888\n",
            "   macro avg       0.47      0.47      0.47      2888\n",
            "weighted avg       0.47      0.48      0.47      2888\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# classification report\n",
        "class_report = classification_report(y_test_labels, predicted_labels)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vjp5aPkNm1oB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJUncFhyWG_T",
        "outputId": "39d4e1c1-3e3a-43fc-e0ca-302e09f05972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91/91 [==============================] - 5s 50ms/step - loss: 1.3325 - accuracy: 0.4782\n",
            "Test Loss: 1.3325, Test Accuracy: 0.4782\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_padded, y_test_labels)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the testing data, the LSTM model gives an accuracy of ~47.82. A higher accuracy for training dataset as compared to that of the test dataset suggests overfitting of the model. To improve accuracy, there are a few solutions. First, embedding with BioWordVec may improve the model's accuracy. Another possible way to improve its accuracy is to include more variables in regard to medical data. This, however may increase computation time. [This](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-022-01665-y) paper\n"
      ],
      "metadata": {
        "id": "8FPtIXXXm2uL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qUKbs7_WTvX"
      },
      "source": [
        "references:\n",
        "\n",
        "https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/#h-lstm-python-for-text-classification\n",
        "\n",
        "paper with potentially more insight of LSTM for medical text and diagnoses [here](https://arxiv.org/abs/1511.03677) and [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8486521/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "DSTA1py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
